{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Natural Language Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zero-shot classification can also be done using [Natural Language Inference (NLI)](https://nlpprogress.com/english/natural_language_inference.html), which refers to the task of determining, given a \"premise\" and a \"hypothesis\", whether the hypothesis is:\n",
        "\n",
        "- True (**entailment**)\n",
        "- False (**contradiction**)\n",
        "- Undetermined (**neutral**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example:\n",
        "\n",
        "| Premise | Hypothesis | Label |\n",
        "| --- | --- | --- |\n",
        "| A soccer game with multiple males playing. | Some men are playing a sport. | entailment |\n",
        "| A man inspects the uniform of a figure in some East Asian country. | The man is sleeping. | contradiction |\n",
        "| An older and younger man smiling. | Two men are smiling and laughing at the cats playing on the floor. | neutral |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NLI (Natural Language Inference) can pull off zero-shot classification by turning the task into a true/false question. Here's how:\n",
        "1. take the text you want to classify (e.g., a movie review) and call it the \"premise.\"\n",
        "2. craft a \"hypothesis\" like, “This is a positive review.”\n",
        "3. The model checks if this hypothesis follows from the premise (entailment = true) or contradicts it (false).\n",
        "    - If it \"entails,\" label it positive;\n",
        "    - if it \"contradicts,\" it’s negative.\n",
        "\n",
        "You don't even need specific training for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zero, single and few-shot classification seem to be an emergent feature of large language models. This feature seems to come about around model sizes of +100M parameters. The effectiveness of a model at a zero, single or few-shot task seems to scale with model size, meaning that larger models (models with more trainable parameters or layers) generally do better at this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Approaches used for NLI include earlier symbolic and statistical approaches to more recent deep learning approaches.\n",
        "- Benchmark datasets used for NLI include [SNLI](https://paperswithcode.com/dataset/snli), [MultiNLI](https://paperswithcode.com/dataset/multinli), [SciTail](https://paperswithcode.com/dataset/scitail), among others.\n",
        "- You can get hands-on practice on the SNLI task by following this [d2l.ai chapter](https://d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's grab a [NLI model from HuggingFace](https://huggingface.co/models?pipeline_tag=zero-shot-classification) and demonstrate how to use it for zero-shot classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -qU datasets transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Pre-trained MNLI model\n",
        "pipe = pipeline(\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device='cuda',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sequence': 'I have a problem with my iphone that needs to be resolved asap!',\n",
              " 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'],\n",
              " 'scores': [0.5227577686309814,\n",
              "  0.45814040303230286,\n",
              "  0.014264780096709728,\n",
              "  0.0026850062422454357,\n",
              "  0.0021520727314054966]}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "predictions = pipe(\"I have a problem with my iphone that needs to be resolved asap!\",\n",
        "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
        ")\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's make it multi-label classification via `multi_labels=True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sequence': 'I have a problem with my iphone that needs to be resolved asap!',\n",
              " 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'],\n",
              " 'scores': [0.9987171292304993,\n",
              "  0.9945850968360901,\n",
              "  0.18989649415016174,\n",
              "  0.0007674150983802974,\n",
              "  0.00038260893779806793]}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "predictions = pipe(\"I have a problem with my iphone that needs to be resolved asap!\",\n",
        "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
        "    multi_label=True\n",
        ")\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try running this on the `rotten_tomatoes` dataset (movie reviews):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "tomatoes = load_dataset(\"rotten_tomatoes\")\n",
        "\n",
        "# Pandas for easier control\n",
        "# tomatoes_train_df = pd.DataFrame(tomatoes[\"train\"])\n",
        "tomatoes_eval_df = pd.DataFrame(tomatoes[\"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It takes 44s to classify all 1,066 examples on a run on T4 GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Candidate labels\n",
        "candidate_labels = [\n",
        "    \"very negative movie review\",\n",
        "    \"very positive movie review\",\n",
        "]\n",
        "candidate_labels_dict = {k: v for k, v in enumerate(candidate_labels)}\n",
        "\n",
        "# Create predictions\n",
        "predictions = pipe(tomatoes_eval_df.text.tolist(), candidate_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sequence': 'I have a problem with my iphone that needs to be resolved asap!',\n",
              " 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'],\n",
              " 'scores': [0.9987171292304993,\n",
              "  0.9945850968360901,\n",
              "  0.18989649415016174,\n",
              "  0.0007674150983802974,\n",
              "  0.00038260893779806793]}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise\n",
        "\n",
        "Identify the user intent using NLI:\n",
        "\n",
        "- `\"Hello, I want to get me a Laptop how much does it cost?\"`\n",
        "    - `\"BUY Laptop\"`\n",
        "- `\"I am very frustrated with your service, and I wanna cancel right now!\"`\n",
        "    - `\"CANCEL Subscription\"`\n",
        "- `\"Here I bought this Keyboard, but it is not working. I want to get my money back!\"`\n",
        "    - `\"REFUND Transaction\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
