{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f855f631",
   "metadata": {},
   "source": [
    "# Arabic Named-Entity Recognition (NER) — Assignment\n",
    "\n",
    "This notebook guides you through building an Arabic NER model using the ANERCorp dataset (`asas-ai/ANERCorp`). Fill in the TODO cells to complete the exercise.\n",
    "\n",
    "- **Objective:** Train a token-classification model (NER) that labels tokens with entity tags (e.g., people, locations, organizations).\n",
    "- **Dataset:** `asas-ai/ANERCorp` — contains tokenized Arabic text and tag sequences.\n",
    "- **Typical Labels:** `B-PER`, `I-PER` (person), `B-LOC`, `I-LOC` (location), `B-ORG`, `I-ORG` (organization), and `O` (outside/no entity). Your code should extract the exact label set from the dataset and build `label_list`, `id2label`, and `label2id` mappings.\n",
    "- **Key Steps (what you will implement):**\n",
    "  1. Load the dataset and inspect samples.\n",
    "  2. Convert the provided words into sentence groupings (use `.` `?` `!` as sentence delimiters) before tokenization so sentence boundaries are preserved.\n",
    "  3. Tokenize with a pretrained Arabic tokenizer and align tokenized sub-words with original labels (use `-100` for tokens to ignore in loss).\n",
    "  4. Prepare `tokenized_datasets` and data collator for dynamic padding.\n",
    "  5. Configure and run model training using `AutoModelForTokenClassification` and `Trainer`.\n",
    "  6. Evaluate using `seqeval` (report precision, recall, F1, and accuracy) and run inference with a pipeline.\n",
    "\n",
    "- **Evaluation:** Use the `seqeval` metric (entity-level precision, recall, F1). When aligning predictions and labels, filter out `-100` entries so only real token labels are compared.\n",
    "\n",
    "- **Deliverables:** Completed notebook with working cells for data loading, tokenization/label alignment, training, evaluation, and an inference example. Add short comments explaining choices (e.g., sentence-splitting strategy, tokenizer settings).\n",
    "\n",
    "Good luck — implement each TODO in order and run the cells to verify output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b101905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install the required packages for Arabic NER with transformers\n",
    "# Required packages: transformers, datasets, seqeval, evaluate, accelerate\n",
    "# Use pip install with -q flag to suppress output\n",
    "\n",
    "#!pip install transformers datasets seqeval evaluate accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c49a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e24f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers datasets evaluate --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: List the files in the current directory to explore the workspace\n",
    "# Hint: Use a simple command to display directory contents\n",
    "!ls -la\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da23007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the ANERCorp dataset and extract label mappings\n",
    "# Steps:\n",
    "# 1. Import required libraries (datasets, numpy)\n",
    "# 2. Load the \"asas-ai/ANERCorp\" dataset using load_dataset()\n",
    "# 3. Inspect the dataset structure - print the splits and a sample entry\n",
    "# 4. Extract unique tags from the training split\n",
    "# 5. Create label_list (sorted), id2label, and label2id mappings\n",
    "import numpy as np\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"asas-ai/ANERCorp\")\n",
    "\n",
    "print(f\"Dataset Split: {dataset}\")\n",
    "print(f\"Sample Entry: {dataset['train'][0]}\")\n",
    "\n",
    "# Collect unique tag strings\n",
    "unique_tags = set()\n",
    "\n",
    "for row in dataset[\"train\"]:\n",
    "    unique_tags.add(row[\"tag\"])\n",
    "       \n",
    "\n",
    "# Create label mappings\n",
    "label_list = sorted(list(unique_tags))\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "print(\"\\nLabel List:\")\n",
    "print(label_list)\n",
    "\n",
    "print(\"\\nlabel2id:\")\n",
    "print(label2id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verify the dataset was loaded correctly\n",
    "# Print the dataframe or dataset summary to inspect the data structure\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb44b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load tokenizer and create tokenization function\n",
    "# Steps:\n",
    "# 1. Import AutoTokenizer from transformers\n",
    "# 2. Set model_checkpoint to \"aubmindlab/bert-base-arabertv02\"\n",
    "# 3. Load the tokenizer using AutoTokenizer.from_pretrained()\n",
    "# 4. Create tokenize_and_align_labels function that:\n",
    "#    - Tokenizes the input text (is_split_into_words=True)\n",
    "#    - Maps tokens to their original words\n",
    "#    - Handles special tokens by setting them to -100\n",
    "#    - Aligns labels with sub-word tokens\n",
    "#    - Returns tokenized inputs with labels\n",
    "# 5. Important: Convert words to sentences using punctuation marks \".?!\" as sentence delimiters\n",
    "#    - This helps the model understand sentence boundaries\n",
    "#    - Hint (suggested approach): group `examples['word']` into sentence lists using \".?!\" as end markers, e.g.:\n",
    "#        sentences = []\n",
    "#        current = []\n",
    "#        for w in examples['word']:\n",
    "#            current.append(w)\n",
    "#            if w in ['.', '?', '!'] or (len(w) > 0 and w[-1] in '.?!'):\n",
    "#                sentences.append(current)\n",
    "#                current = []\n",
    "#        if current:\n",
    "#            sentences.append(current)\n",
    "#      Then align `examples['tag']` accordingly to these sentence groups before tokenization.\n",
    "# 6. Apply the function to the entire dataset using dataset.map()\n",
    "\n",
    "\n",
    "# def tokenize_and_align_labels(examples):\n",
    "#     # TODO: Implement tokenization and label alignment\n",
    "#     # Hint: Use tokenizer with is_split_into_words=True\n",
    "#     # Handle -100 for special tokens and sub-words\n",
    "#     # Note: Consider punctuation marks \".?!\" when processing sentence boundaries\n",
    "#     pass\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"aubmindlab/bert-base-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "  \n",
    "    words = examples[\"word\"]\n",
    "    tags = examples[\"tag\"]\n",
    "    \n",
    "    sentences = []\n",
    "    sentence_tags = []\n",
    "    current_sentence = []\n",
    "    current_sentence_tags = []\n",
    "    \n",
    "    for word, tag in zip(words, tags):\n",
    "        current_sentence.append(word)\n",
    "        current_sentence_tags.append(tag)\n",
    "        \n",
    "        if word in [\".\", \"?\", \"!\"] or (len(word) > 0 and word[-1] in \".?!\"):\n",
    "            sentences.append(current_sentence)\n",
    "            sentence_tags.append(current_sentence_tags)\n",
    "            current_sentence = []\n",
    "            current_sentence_tags = []\n",
    "    \n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        sentence_tags.append(current_sentence_tags)\n",
    "    \n",
    "    tokenized_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    \n",
    "    for sentence, tags_list in zip(sentences, sentence_tags):\n",
    "        tokenized = tokenizer(\n",
    "            sentence,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        word_ids = tokenized.word_ids()\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[tags_list[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        tokenized_inputs[\"input_ids\"].append(tokenized[\"input_ids\"])\n",
    "        tokenized_inputs[\"attention_mask\"].append(tokenized[\"attention_mask\"])\n",
    "        tokenized_inputs[\"labels\"].append(label_ids)\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"Tokenization complete!\")\n",
    "print(f\"Tokenized train samples: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"Tokenized test samples: {len(tokenized_datasets['test'])}\")\n",
    "\n",
    "# Verify \n",
    "print(\"\\n Sample tokenized entry:\")\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "print(f\"Input IDs length: {len(sample['input_ids'])}\")\n",
    "print(f\"Labels length: {len(sample['labels'])}\")\n",
    "print(f\"First few labels: {sample['labels'][:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df25b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"aubmindlab/bert-base-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "\n",
    "    for words, tags in zip(examples[\"word\"], examples[\"tag\"]):\n",
    "        current_words = []\n",
    "        current_tags = []\n",
    "\n",
    "        \n",
    "        sentences = []\n",
    "        sentence_tags = []\n",
    "        for w, t in zip(words, tags):\n",
    "            current_words.append(w)\n",
    "            current_tags.append(t)\n",
    "            if w in [\".\", \"?\", \"!\"] or (len(w) > 0 and w[-1] in \".?!\"):\n",
    "                sentences.append(current_words)\n",
    "                sentence_tags.append(current_tags)\n",
    "                current_words = []\n",
    "                current_tags = []\n",
    "        if current_words:\n",
    "            sentences.append(current_words)\n",
    "            sentence_tags.append(current_tags)\n",
    "\n",
    "        \n",
    "        for sent_words, sent_tags in zip(sentences, sentence_tags):\n",
    "            tokenized = tokenizer(\n",
    "                sent_words, is_split_into_words=True, truncation=True\n",
    "            )\n",
    "            word_ids = tokenized.word_ids()\n",
    "            label_ids = []\n",
    "            previous_word_idx = None\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label2id[sent_tags[word_idx]])  \n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "            tokenized_inputs[\"input_ids\"].append(tokenized[\"input_ids\"])\n",
    "            tokenized_inputs[\"attention_mask\"].append(tokenized[\"attention_mask\"])\n",
    "            tokenized_inputs[\"labels\"].append(label_ids)\n",
    "\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edd8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the compute_metrics function for model evaluation\n",
    "# Steps:\n",
    "# 1. Import evaluate and load \"seqeval\" metric\n",
    "# 2. Create compute_metrics function that:\n",
    "#    - Extracts predictions from model outputs using argmax\n",
    "#    - Filters out -100 labels (special tokens and sub-words)\n",
    "#    - Converts prediction and label IDs back to label names\n",
    "#    - Computes seqeval metrics (precision, recall, f1, accuracy)\n",
    "#    - Returns results as a dictionary\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "seqeval = seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \n",
    "    predictions, labels = p.predictions, p.label_ids\n",
    "\n",
    "   \n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    \n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for pred_row, label_row in zip(preds, labels):\n",
    "        row_labels = []\n",
    "        row_preds = []\n",
    "        for p_id, l_id in zip(pred_row, label_row):\n",
    "            if l_id != -100:  # ignore special tokens\n",
    "                row_labels.append(id2label[l_id])\n",
    "                row_preds.append(id2label[p_id])\n",
    "        true_labels.append(row_labels)\n",
    "        true_predictions.append(row_preds)\n",
    "\n",
    "    # Compute seqeval metrics\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daefa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,          \n",
    "    num_labels=len(label_list),  \n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"arabert-ner\",          \n",
    "    eval_strategy=\"epoch\",               \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"overall_f1\"  \n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "train_val_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_val_split[\"train\"],      \n",
    "    eval_dataset=train_val_split[\"test\"],        \n",
    "    processing_class=tokenizer,  \n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the trained model with inference\n",
    "# Steps:\n",
    "# 1. Import pipeline from transformers\n",
    "# 2. Create an NER pipeline using the trained model and tokenizer\n",
    "# 3. Use aggregation_strategy=\"simple\" to merge sub-tokens back into words\n",
    "# 4. Test the pipeline with an Arabic text sample\n",
    "# 5. Pretty print the results showing entity, label, and confidence score\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "ner_pipeline= pipeline(\n",
    "    \"ner\",\n",
    "    model=trainer.model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "text = \"أعلن المدير التنفيذي لشركة أبل تيم كوك عن افتتاح فرع جديد في الرياض.\"\n",
    "results = ner_pipeline(text)\n",
    "\n",
    "for entity in results:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
